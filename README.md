# Agente Conversacional CLI con Memoria (Hugging Face)

Un agente conversacional de l√≠nea de comandos que utiliza modelos de lenguaje abiertos de Hugging Face e implementa un sistema de memoria persistente para mantener contexto entre conversaciones.

## üöÄ Caracter√≠sticas

- **Memoria de Corto Plazo**: Mantiene las √∫ltimas 10 interacciones en memoria RAM para contexto inmediato
- **Memoria de Largo Plazo**: Persiste todo el historial de conversaci√≥n en archivo JSON local
- **Modelos Abiertos**: Utiliza modelos de Hugging Face (DialoGPT, BlenderBot, etc.)
- **Ejecuci√≥n Local**: Funciona completamente offline despu√©s de la descarga inicial
- **Interfaz CLI Interactiva**: Interfaz de terminal amigable con comandos especiales
- **Optimizaci√≥n Autom√°tica**: Detecci√≥n de GPU/CPU y manejo inteligente de memoria
- **Recuperaci√≥n de Sesi√≥n**: El agente recuerda conversaciones anteriores al reiniciar

## üìã Requisitos del Sistema

### Requisitos M√≠nimos
- Python 3.8+
- 4GB RAM
- 2GB espacio libre (para descargar modelos)
- Conexi√≥n a internet (solo para descarga inicial)

### Requisitos Recomendados
- Python 3.9+
- 8GB+ RAM
- GPU con CUDA (opcional, mejora velocidad significativamente)
- 5GB espacio libre

## üõ†Ô∏è Instalaci√≥n

### 1. Clonar o descargar el proyecto
```bash
git clone <url-del-repositorio>
cd agente-conversacional-hf
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```

**Para usuarios con GPU NVIDIA (recomendado):**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. Primera ejecuci√≥n
```bash
python main.py
```

**Nota**: La primera vez descargar√° el modelo (~1GB-3GB) autom√°ticamente.

## üéØ Uso

### Ejecutar el agente
```bash
python main.py
```

### Comandos disponibles

Durante la conversaci√≥n, puedes usar estos comandos especiales:

- `exit` o `quit` - Terminar la aplicaci√≥n
- `clear` - Limpiar memoria de corto plazo
- `history` - Mostrar las √∫ltimas 5 interacciones
- `info` - Mostrar informaci√≥n del modelo cargado

```

## üèóÔ∏è Arquitectura del Sistema

### Estructura del proyecto
```
proyecto/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n del modelo HF y par√°metros
‚îÇ   ‚îú‚îÄ‚îÄ memory_system.py       # Sistema de memoria (sin cambios)
‚îÇ   ‚îú‚îÄ‚îÄ huggingface_client.py  # Cliente para modelos de Hugging Face
‚îÇ   ‚îî‚îÄ‚îÄ cli_interface.py       # Interfaz CLI adaptada para HF
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ conversation_history.json  # Historial persistente
```

### Componentes principales

#### 1. Sistema de Memoria (`memory_system.py`)
- **Sin cambios** respecto a la versi√≥n Azure OpenAI
- Memoria de corto y largo plazo funcional

#### 2. Cliente Hugging Face (`huggingface_client.py`)
- **Carga autom√°tica de modelos**: Descarga y cache local
- **Detecci√≥n de hardware**: GPU/CPU autom√°tico
- **Generaci√≥n optimizada**: Control de temperatura, top-p, etc.
- **Manejo de contexto**: Truncado inteligente para evitar l√≠mites
- **Limpieza de respuestas**: Filtrado de repeticiones y formato

#### 3. Interfaz CLI Mejorada (`cli_interface.py`)
- **Verificaci√≥n de dependencias**: Comprueba PyTorch/Transformers
- **Informaci√≥n del modelo**: Comando `info` para detalles t√©cnicos
- **Mejor feedback**: Indicadores de descarga y carga

#### 4. Configuraci√≥n (`config.py`)
- **Modelos configurables**: F√°cil cambio entre modelos
- **Par√°metros ajustables**: Temperatura, longitud, etc.
- **L√≠mites inteligentes**: Manejo de memoria y contexto

## ü§ñ Modelos Disponibles

### Modelos Recomendados (configurables en `config.py`):

```python
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
MODEL_NAME = "google/gemma-2b-it"
```

## üîß Decisiones T√©cnicas

### Selecci√≥n de Modelo
- **Qwen2.5-1.5B-Instruct**: Balance entre calidad y recursos

### Optimizaciones de Rendimiento
- **Truncado inteligente**: Mantiene contexto relevante
- **Cache local**: Los modelos se descargan una sola vez
- **Generaci√≥n controlada**: Evita respuestas muy largas

### Manejo de Memoria
- **Detecci√≥n autom√°tica**: GPU vs CPU
- **L√≠mites din√°micos**: Ajustados seg√∫n el dispositivo
- **Limpieza autom√°tica**: Liberaci√≥n de memoria entre respuestas

## üìä Rendimiento Esperado

### En CPU (sistema t√≠pico):
- **Carga inicial**: 1-3 minutos
- **Respuesta**: 30-120 segundos
- **Memoria RAM**: 6-8 GB

## üêõ Troubleshooting

### Problemas comunes

#### "Error cargando modelo"
```bash
# Soluci√≥n 1: Actualizar transformers
pip install --upgrade transformers torch

# Soluci√≥n 2: Limpiar cache
rm -rf ~/.cache/huggingface/transformers/
```

#### "CUDA out of memory"
- Cambiar a modelo m√°s peque√±o en `config.py`
- Usar CPU: `CUDA_VISIBLE_DEVICES="" python main.py`
- Reducir `MAX_LENGTH` en config

#### "Respuestas de baja calidad"
- Probar diferentes modelos en `config.py`
- Ajustar `TEMPERATURE` (0.7-0.9)
- Usar comando `clear` para limpiar contexto

#### "Demora en primera carga"
- Normal: descarga modelo desde internet
- Verificar espacio en disco (~2-5GB)
- Paciencia en la primera ejecuci√≥n

## üìù Configuraci√≥n Personalizada

Para cambiar modelos o par√°metros, edita `src/config.py`:

```python
# Cambiar modelo
MODEL_NAME = "modelo-preferido"

# Ajustar par√°metros de generaci√≥n
TEMPERATURE = 0.8      # M√°s creativo
MAX_LENGTH = 200       # Respuestas m√°s largas
TOP_P = 0.95          # M√°s diversidad
```

---
